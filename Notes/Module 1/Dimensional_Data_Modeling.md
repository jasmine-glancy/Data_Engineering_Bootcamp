# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Dimensional Data Modeling

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Complex Data Type and Cumulation Day 1 Lecture

| Concept               | Notes                                                                                 |
|------------------------|----------------------------------------------------------------------------------------|
| **Complex Data Types** | - Structs = nested tables <br> - Arrays = lists inside columns <br> - Compact but harder to query <br> - Use based on customer needs |
| **Dimensions**         | - Are attributes of an entity (such as the user’s birthday, user’s favorite food, etc). <br> - Not critical to the identification of an entity, but help provide information <br> - Dimensions can be **slowly-changing** or **fixed**
| **Slowly-Changing Dimensions**  | - Are *time-dependent* and can change as time goes on |
| **Knowing Your Customer** | ***Think about the down-stream user!*** <br> Some of the biggest problems in data engineering occur when the data is modeled for the wrong customer. <br><br>- Data analysts/Data scientists <br>  &emsp;• Should be very easy to query. <br>  &emsp;• Not many complex data types <br>  &emsp;• Use flat data sets <br>  &emsp;• You don't want to make their job needlessly difficult!<br> - Other data engineers <br> &emsp;• Should be compact and probably harder to query. <br> &emsp;• Nested types are ok <br> &emsp;• Used to more complex data types<br> - ML models <br> &emsp;• Depends on the model and how it is trained <br> - Customers <br>&emsp;• Should be a very easy to interpret chart |
| **Master Data**  | - Data that other data engineers and other people in the company depend on <br>  &emsp;• Connected to other data sets |
| **Online Transaction Processing (OLTP)**  | **IS A CONTINUUM** <br><br>- Optimizes for low-latency, low-volume series <br>- How software engineers do their data modeling to make their online systems run ***as quickly as possible*** <br> - MySQL and Postgres <br> - Require a lot of JOINS to get what you want <br> - ***Looking at one entity*** |
| **Online Analytical Processing (OLAP)**  | **IS A CONTINUUM** <br><br>- Optimizes for large volume <br>- GROUP BY queries <br>- Minimizes JOINS <br> - Most common data modeling that data engineers use <br> - "Can we run a query that's fast and you don't need a lot of joins?" <br>- ***Looks at the entire data set*** or at least a chunk|
| **OLAP Cubes**  | - You flatten the data out <br>- You can then aggregate and query <br> -i.e. "slice and dice" <br>-  Data scientists and analysts benefit from OLAP cubes because they provide an easy-to-use format for quick analytical queries without needing complex joins.|
| **Master Data**  | - Middle ground between OLTP and OLAP <br>- Entities should be de-duped with complete definitions |
| **Metrics**  | - Left over after aggregating down the OLAP cube <br>- The individual value<br>  &emsp;• Distilled version of the original data |
| **Cumulative Table Design**  | - Core components <br>  &emsp;• 2 Dataframes (yesterday and today)<br>  &emsp;• FULL OUTER JOIN the two data frams together<br>  &emsp;• COALESCE values to keep everything around <br>  &emsp;&emsp;• Some data may or may not match!<br>  &emsp;• *Provides a complete history* <br>- Usages <br>  &emsp;• Growth analytics <br>  &emsp;&emsp;• If a user was active yesterday but not today, they are *churned* <br>  &emsp;&emsp;• If they were not active yesterday and they are active today, they are *resurrected* <br>  &emsp;• State transition tracking <br>- Steps: <br>  &emsp;1. Yesterday + Today <br> &emsp; 2. FULL OUTER JOIN <br>  &emsp;3. COALESCE id and unchanging dimensions <br>  &emsp;4. Compute cumulation metrics (e.g. days since x) <br>  &emsp;5. Combine arrays and changing values <br>- The cumulated output for today will be yesterday's input! <br>- **Strengths** <br>&emsp;• Historical analysis without GROUP BY <br>&emsp;• Allow for scalable queries <br>&emsp;• Easy "transition" analysis <br>- **Drawbacks** <br>&emsp;• Can only be backfilled sequentially <br>&emsp;•Handling PII data can be a mess since deleted/inactive users get carried forward <br>&emsp;• Relies on yesterday's data for continuity, which makes parallel backfilling difficult |
| **Compactness vs Usability Tradeoff**  | - Most tables usually <br>&emsp;• Have no complex data types <br>&emsp;• Easily can be manipulated with WHERE and GROUP BY <br>- Most compact tables <br>&emsp;• Are not human readable <br>&emsp;• Are compressed to be as small as possible and can't be queried directly until they are decoded<br>- Middle-ground tables <br>&emsp;• Use complex data types (i.e. ARRAY, MAP, and STRUCT) <br>&emsp;• Make queries trickier, but compact the data more <br>- Very usable tables are more analytics-focused, while the more compact tables are going to be more software-engineering or production data-focused <br>- *When to use each table* <br>&emsp;• **Most compact** <br>&emsp;&emsp;• Online systems where latency and data volumes matter a lot <br>&emsp;&emsp;• Customers are usually highly technical<br>&emsp;• **Middle-ground** <br>&emsp;&emsp;• Upstream staging <br>&emsp;&emsp;• Master data where the majority of consumers are other data engineers <br>&emsp;• **Most Usable (i.e. OLAP)**<br>&emsp;&emsp;• When analytics is the main consumer and the majority of consumers are less technical|
| **Structs vs Array vs MAP**  | - **Structs** <br>&emsp;• A table within a table <br>&emsp;• Keys are rigidly defined. Compression is good! <br>&emsp;• Values can be any type <br>- **Array** <br>&emsp;• Good for ordered data sets  <br>&emsp;• Can be nested<br>&emsp;• A list of values that all have to be the same type <br>- **Map** <br>  &emsp;• Keys are loosely defined. Compression is okay! <br>&emsp;&emsp;• Keys are not limited <br>&emsp;• ***Values all have to be the same data type!***  |
| **Temporal Cardinality Explosions of Dimensions**  | - When you add a temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude <br>- Example: <br>&emsp;• Airbnb has ~6 million listings <br>&emsp;&emsp;• If we want to know the nightly pricing and availability of each night for the next year  <br>&emsp;&emsp;&emsp;• That's 365 * 6 million or about ~1 billion nights  <br>&emsp;&emsp;• Should this dataset be...  <br>&emsp;&emsp;&emsp;• Listing-level with an array of nights? <br>&emsp;&emsp;&emsp;• Listing night level with 2 billion rows? <br>&emsp;&emsp;• If you do the sorting right, Parquet will keep these two about the same size <br><br> **Denormalized Temporal Dimensions** <br>- If you explode out your data and need to join other dimensions, *Spark shuffle will ruin your compression!*|
| **Run-Length Encoding Compression**  | - Probably the most important compression technique in big data right now <br>  &emsp;• It's why Parquet file format has become so successful <br> - If you have a bunch of duplicate data, it's nullified <br>  &emsp;• ***Nullifies everything except for the first value and stores the value of the duplicates***<br>- Suffle can ruin this. ***Be careful!***|
| **Spark Suffle**  | - After a JOIN, Spark may mix up the ordering of your rows and ruin your compression <br> &emsp;• Shuffle happens in distributed environments when you do JOIN and GROUP BY  <br>&emsp;• If you model with complex data types, the data will already be sorted for your downstream users! |



## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What are 'struct' and 'array' considered as in dimensional data modeling?
- What is a characteristic of a slowly changing dimension?
- What is the purpose of the cumulative table design in data modeling?
- In the context of dimensional data modeling, who would likely benefit the most from an OLAP cube?
- What is a potential drawback of cumulative table design?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

xxx
