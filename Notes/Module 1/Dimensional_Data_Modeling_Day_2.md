# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Dimensional Data Modeling

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Building Slowly Changing Dimensions Day 2 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Slowly Changing Dimensions**  | - An attribute that drifts overtime<br>  &emsp;• i.e. your favorite food<br>  &emsp;• Have a time frame associated with them <br>  &emsp;• Some dimensions change, some don't <br>  &emsp;• ***If you don't model SCDs the right way, they impact idempotency!***|
| **Idempotency**  | - The ability of your data pipelines to produce the same results whether it's running in production or backfill <br>  &emsp;• *VERY important property of pipelines in order to enforce data qualiy!*<br>  &emsp;• ***Your pipeline better produce the same results regardless of when or how many times it's ran!!!*** <br>- If you can build idempotent pipelines, you will be a **MUCH** better data engineer! <br>- **Technical definition:** Denotes an element of a set which is unchanged in value when multiplied or otherwise operated on by itself |
| **Why Troubleshooting Non-Idempotent Pipelines is Hard**  | - Silent failure! <br>  &emsp;• It doesn't give you errors, it just produces ***non-reproducable*** data <br>- You only see it when you get data inconsistencies and a data analyst brings it up <br>- **If you have downstream data engineers that are depending on your data, and that data isn't idempotent, their data will also not be idempotent!**|
| **What Can Make A Pipeline Not Idempotent**  | 1. `INSERT INTO` without `TRUNCATE` <br>  &emsp;• If you are running the data for a day, you run the pipeline once, but when you run it a second time there is twice as much data <br>  &emsp;• If you don't have a `TRUNCATE` statement that clears out the data you're about to insert, then you're just going to keep duplicating the data! <br>  &emsp;• **Use `MERGE` or `INSERT OVERWRITE` every time, please** <br>&emsp;&emsp;• `MERGE` checks if the data is the same and only writes the data that doesn't already exist yet<br>&emsp;&emsp;• `INSERT OVERWRITE` overwrites whatever data already exists<br>2. Using `start_date >` without a corresponding `end_date <` <br>&emsp;• i.e. if you have a pipeline with a `WHERE` clause and the date is greater than yesterday, and you run it today, you will get one day of data. <br>&emsp;&emsp;• But if you run it tomorrow, you will get two days worth of data<br>&emsp;&emsp;• Each consecutive day you run this, you will get one more day of data each time <br>&emsp;&emsp;• ***This is an example of an indempotent pipeline!!!***<br>&emsp;&emsp;• Create a window instead (i.e. the pipeline processes a few days of data)<br>&emsp;&emsp;• You can create out of memory exceptions when you backfill with this method<br>3. Not using a full set of partition sensors <br>&emsp;&emsp;• i.e. an incomplete set of data or runs before all inputs are ready<br>&emsp;&emsp;• When you backfill, the data is there, but in production can fire too early when it is still missing the input sets <br>&emsp;&emsp;• This causes a difference between the backfill and production results <br> &emsp;• The pipeline might run when there is no or partial data <br>4. Not using *depends_on_past* for cumulative pipelines <br>&emsp;• AKA **Sequential processing**<br>&emsp;&emsp;• If you have a pipeline that depends on yesterday's data, that means the pipeline can't run in parallel. <br>&emsp;&emsp;• It has to run yesterday, then today, then the next day<br>&emsp;&emsp;• Most pipelines aren't like this, but you can't do this with cumulative pipelines<br>&emsp;• If you don't use sequential processing for cumulative pipelines, te pipeline can end up reading yesterday's data when it hasn't been created yet <br>&emsp;&emsp;• This will start the cumulation over and you will only have today's data <br>- ***THE PRODUCTION AND BACKFILL DATA SHOULD ALWAYS BE THE SAME***<br>5. Relying on the "latest" partition of a not properly modeled SCD table <br>&emsp;• Cumulative table design *amplifies* this bug!<br>6. Relying on the "latest" partition of anything else <br>&emsp;• **Exception for non-production:** If you are backfilling data and have a properly modeled SCD (slowly changing dimension) table, then you can rely on your latest data <br>- If you have a non-idempotent pipeline that gives inconsistent data and a cumulative pipeline that depends on this data, ***this cumulative pipeline will carry the bugs forward every day***!|
| **The Pains of Not Having Idempotent Pipelines**  | - Backfilling causes inconsistencies between the old and restated data <br>- Very hard to troubleshoot bugs <br>- Unit testing cannot replicate the production behavior<br> &emsp;• **If your pipeline is not idempotent, your unit tests *will still pass!***<br> &emsp;• Unit tests are better with idempotent pipelines<br>- Silent failures <br> &emsp;• Upon backfill <br> &emsp;• Upon restating <br> &emsp;• One of the most painful parts of data engineering |
| **Modeling Slowly Changing Dimensions (SCD)**  | - What are the options? <br>  &emsp;• Latest snapshot<br> &emsp;&emsp;• Instead of modeling day-by-day, you use the current value<br> &emsp;&emsp;• But if you have a SCD and you only hold the latest value, **the pipeline is non-indempotent!**<br>  &emsp;• Daily/monthly/yearly<br> &emsp;• Slowly changing dimensions <br> &emsp;&emsp;• Check for change day-over-day<br>- Most dimensions are SCDs. <br>- **Rapidly-changing dimensions** also exist (i.e. heart rate) <br>- The slower-changing it is, the better it is <br>  &emsp;• You will get the most efficiency gains with these <br>- How slowly changing are the dimensions you're modeling?<br><br>- **Three ways to model dimensions that change:** <br>1. Singular snapshots <br> &emsp;• ***Remember:*** These are not idempotent!<br>2. Daily partitioned snapshots<br>3. SCD types 0, 1, 2, and 3 |
| **Types of Slowly Changing Dimensions**  | - **Type 0** <br> &emsp;• Aren't actually slowly changing (i.e. birth day)<br>&emsp;&emsp;• **Is idempotent because the values are unchanging** <br> &emsp;• Modeled as type 0 when you know something absolutely will not change <br> &emsp;• You don't need a temporal component<br><br>- **Type 1** <br> &emsp;• You only care about the latest value <br> &emsp;• ***NEVER USE THIS TYPE FOR ANALYTICS BECAUSE IT MAKES YOUR PIPEINES NOT IDEMPOTENT ANYMORE***<br> &emsp;• For Online Transactional Processing (OLTP), using the latest value is fine if you only care about the latest value <br>&emsp;&emsp;• If you backfill with this dataset, you will get the dimension as it is now, not as it was then!<br><br>- **Type 2** <br> &emsp;• AirBnB calls this the "gold standard" <br> &emsp;• You care about what the value was from `start_date` to `end_date`<br> &emsp;• Outputs 2 rows with the query's values at various points in time<br> &emsp;• Current values usually have either an `end_date` that is<br> &emsp;&emsp;• `NULL`<br> &emsp;&emsp;•Far into the future like 9999-12-31<br> &emsp;• Hard to use <br> &emsp;&emsp;• Since there's more than 1 row per dimension, you need to be careful about filtering on time<br> &emsp;• ***A  purely idempotent SCD***<br> &emsp;&emsp;• They don't lose any clarity <br><br>- **Type 3**<br>  &emsp;• You only care about "original" and "current"<br>  &emsp;• Benefits<br>&emsp;&emsp;• You only have 1 row per dimension<br>&emsp;&emsp;• Nice because you don't have to filter<br>  &emsp;• Drawbacks<br>&emsp;&emsp;• You lose history between original and current<br>&emsp;&emsp;• *Does not store WHEN things change*<br>  &emsp;• Is this idempotent?<br>&emsp;&emsp;• Partially (so **no**)|
| **Slowly Changing Dimension Type 2 Loading**  | - **Two ways to accomplish:** <br>&emsp;&emsp;1. Loads the entire history in one query <br>&emsp;&emsp;&emsp;• Inefficient, but nimble <br>&emsp;&emsp;&emsp;• 1 query and you're done <br>&emsp;&emsp;2. Incrementally load the data after the previous SCD is generated <br>&emsp;&emsp;&emsp;• Has the same "depends_on_past" constraint <br>&emsp;&emsp;&emsp;• Efficient, but cumbersome <br>&emsp;&emsp;&emsp;• Generally speaking, you want to have your production run be the later so you don't have to process the whole history <br>- These options can perform about the same depending on your needs <br>- You can make pipelines as efficient as possible, but you should also consider how you can contribute the most value to your company <br>  &emsp;• Ask yourself what the opportunity cost is |

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What are SCDs in data modeling?
- Why is it important to model Slowly Changing Dimensions correctly?
- What is idempotency?
- What are the best practices when building pipelines?
- What can make a pipeline idempotent?
- What makes Type 2 Slowly Changing Dimensions advantageous, as mentioned in the lecture?
- Which of the following is NOT a type of Slowly Changing Dimension according to the lecture?
- What problem arises from using the 'latest snapshot' approach to SCD, based on the lecture?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

SCDs are Slowly Changing Dimensions that change over time. It's important to model SCDs correctly so you get the same results no matter when or where you run your pipeline, i.e. idempotency. When updating data, use `INSERT OVERWRITE` or `MERGE` to ensure things do not duplicate.

SCDs can be type 0, 1, 2, or 3. Type 2 Slowly Changing Dimensions are advantageous because they use start and end dates, allowing them to track and store the history of changes over time, ensuring accurate historical data representation.

Using the "latest snapshot" approach can result in incorrect data when backfilling because it applies the current dimension value to historical data, which can misrepresent past events.
