# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Streaming Pipelines

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Setting up Streaming Pipelines Day 1 Lab

| Concept                | Notes            |
|---------------------|------------------|
| **Set Up** | - Clone the latest version of the repo <br>- Bootcamp -> materials -> 4-apache-flink-training<br>- Copy environmental variables in 0:54 into the .env file<br>- Go to IP2 location for an API key <br>- Rename the file `flink-env.env`<br>- Use the `docker compose` command to make the Docker image <br>- start_job.py <br> &emsp;• Every time you load a page for Data Expert, it puts a record into Kafka <br> &emsp;• Kafka topic is called bootcamp-events-prod<br> &emsp;&emsp;• Takes the data from Kafka <br> &emsp;&emsp;• Hits the API location <br> &emsp;&emsp;• Geocodes the IP address so we can see where the traffic is coming from<br> &emsp;• Go to localhost:8081 <br> &emsp;&emsp;• You will see the flink job manager <br>- **Important**<br> &emsp;• Make sure you have your container from week 1 running <br> &emsp; •Go into Postgres <br> &emsp;&emsp;• Run init.sql to get Postgres to collect the data <br>- You will get a job ID when the job has been successfully submitted <br> - If `SELECT * FROM processed_events` returns data, you are successful! |
| **start_job.py**  | - log_processing starts the job and grabs the information<br>- Gives Flink the ability to set up a streaming environment by default <br> &emsp;• You can also enable batch_mode <br>- Adjust to `env.enable_checkpointing(10 * 1000)` for every 10 seconds <br> &emsp;• Every 10 miliseconds is too many checkpoints <br>- `t_env.create_temporary_function("get_location", get_location)` is very similar to registering a user-defined function in Spark<br> &emsp;• `GetLocation(ScalarFunction)` class takes the location from the IP2Location website and uses the IP address and gets the country, region, and city of the data provided<br> &emsp;&emsp;• If you don't give it a key, the request fails and returns an empty dictionary<br> &emsp;&emsp;• Scalar functions take in one row or one column and return one row or one column<br>- `WATERMARK FOR event_timestamp AS event_timestamp - INTERVAL '15' SECOND`<br> &emsp;• Watermark gets a 15 second window that will fix the ordering of events<br>- Kafka and Flink are not connected, but you can make them connected<br> &emsp;• A Kafka topic is similar to a database table, and a Kafka group is similar to a schema <br>- `'scan.startup.mode' = 'latest-offset'` <br> &emsp;• When you first kick off your flink job, this is going to read from the first record in Kafka or the last record in Kafka<br>- `'properties.auto.offset.reset' = 'latest'`<br> &emsp;• If the job fails and restarts, do we read from the first offset or the last offset? <br> &emsp;&emsp;• Or you can choose checkpointing, but checkpointing is optional<br>- `'format' = 'json'` chooses the output type<br>- When you are writing records to Postgres, you want to include a `.wait()` at the end of the last line otherwise it will run for every record that is on the top of the queue, then it will end the job <br> &emsp;• `.wait()` makes it run continuously |
| **Reset Docker**  | - Kill the image with `make down` or `docker compose down --remove-orphans`<br>- Spin it up again with `docker compose --env-file flink-env.env up --build --remove-orphans -d` |
| **Task Managers and Jobs**  | - On localhost:8081, you should see items in task managers <br>- If you go to jobs, you will see your task slots<br> &emsp;• Task slots tell you how many jobs you can run at once <br> &emsp;• If you have more jobs than your slots, the next jobs scheduled will wait for the previous jobs to finish<br> &emsp;&emsp;• Your jobs will not finish if they are streaming jobs <br>- `make job`/ `docker compose exec jobmanager ./bin/flink run -py /opt/src/job/start_job.py --pyFiles /opt/src -d` <br> &emsp;• Calls up the Docker container again which should submit 2 jobs <br> &emsp;&emsp;• These are the jobs to write to Kafka and the job to write to Postgres |
| **Process Events**  | - `SELECT geodata::json->>'country', COUNT(1) FROM processed_events LIMIT 10` <br> &emsp;• Takes the country out of the json results and tallies the users for each country<br>- Busy <br> &emsp;• Means we are at maximum computation, and the Flink job can't process more data<br>- Backpressure <br> &emsp;• If the Flink job gets more data past "busy", it moves to backpressure<br> &emsp;• Tells Kafka to slow down and add latency to the data so we don't out of memory <br> &emsp;• Can happen if the data goes viral or your data set gets a lot of hits<br>- Kafka sinks are more performant because they are meant for high throughput |

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What is the first step in setting up the Flink Lab environment?
- How can Apache Flink read data from different sources?
- What type of UDF is 'get location' in Flink?
- Why is it important to not publish Kafka credentials publicly?
- How does 'back pressure' function in a Flink job?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

Cloning the data engineer handbook is the first step to access all necessary materials for the Apache Flink training. Docker is used as well. Apache Flink utilizes connectors like Kafka and JDBC to read data from various sources. A Scalar function takes one input and gives one output, fitting the behavior of "get location" which porcesses individual IP addresses.

Kafka credentials are sensitive, and exposing them can result in unauthorized data access or misuse. "Back pressure" in Flink regulates the data flow to manage compute resources efficiently, avoiding memory overload by slowing down the data input.
