# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Streaming Pipelines

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Mastering Streaming and Real-time Pipelines Day 1 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Streaming Pipelines**  | - Streaming pipelines process data in a low-latency way <br>- **Streaming vs near real time vs real time** <br> &emsp;• Streaming (or continuous)<br> &emsp;&emsp;• Data is processed as it is generated<br> &emsp;&emsp;• Example: Flink <br> &emsp;• Near real-time<br> &emsp;&emsp;• Data is processed in small batches every few minutes<br> &emsp;&emsp;• Example: Spark Structured Streaming<br> &emsp;• Real-time and streaming are often synonymous, but not always!<br> &emsp;• Less technical people may think real-time and batch are synonymous <br> &emsp;• **Microbatching** is synonymous with near real-time. Data is processed in small batches |
| **Real-time From Stakeholder POV**  | - Rarely means streaming <br>- Usually means low-latency or predictable refresh rate |
| **When Streaming Should Be Used**  | - Considerations <br> &emsp;• Skills on the team<br> &emsp;• What is the incremental benefit?<br> &emsp;• Homogeneity of your pipelines<br> &emsp;• The tradeoff between daily batch, hourly batch, microbatch, and streaming<br> &emsp;• How should data quality be inserted<br> &emsp;&emsp;• Batch pipelines have easier DQ paths |
| **Streaming-only Use Cases**  | - **Key: *Low latency makes or breaks the use case*** <br>- Examples: <br> &emsp;• Detecting fraud, preventing bad behavior <br> &emsp;• High-frequency trading <br> &emsp;• Live event processing |
| **Gray-area Use Cases**  | - Data that is served to customers<br>- Reducing the latency of upstream master data <br> &emsp;• Notifications dataset had a 9 hour after midnight latency<br> &emsp;• Micro batch cut to 1 hour |
| **No-go Streaming Use Cases**  | - Batch should be used instead <br>- Ask the question <br> &emsp;• What is the incremental benefit of reduced latency? <br>- Analyst complain that the data isn't up-to-date <br> &emsp;• Yesterday's data by 9am is good enough for *most* analytical use cases|
| **How Streaming Pipelines Are Different From Batch Pipelines**  | - *Streaming pipelines run 24/7*! Batch pipelines run for a small percentage of the day <br>- Streaming pipelines are much more software engineering oriented <br> &emsp;• They act more like servers than DAGs<br>- Streaming pipelines need to be treated as such and have more unit test and integration test coverage like servers! |
| **The Streaming->Batch Continuum**  | ![Streaming Pipeline Continuum](streaming_pipelines.png)<br>- The lower the latency, the higher the engineering complexity <br>- Real time is a myth!<br> &emsp;• You'll have seconds of latency just for the event generation -> Kafka -> Flink -> sink <br>- Pipelines can be broken into 4 categories <br> &emsp;• Daily batch<br> &emsp;• Hourly batch (sometimes called near real-time)<br> &emsp;• Microbatch (sometimes called near real-time)<br> &emsp;• Continuous processing (usually called real-time) |
| **Structure of a Streaming Pipeline**  | - **The source** <br> &emsp;• Examples: Kafka, RAbbitMQ<br> &emsp;&emsp;• Enriched dimensional sources (i.e. side inputs)<br> &emsp;&emsp;• Denormalization of fact data or bringinging in other SCDs<br> &emsp;&emsp;• Data can refresh on a cadence to get the right data for each events<br>- **The compute engine** <br> &emsp;• Examples<br> &emsp;&emsp;• Flink <br> &emsp;&emsp;• Spark structured streaming<br> &emsp;• These engines make sense of the incoming streams of data <br>- **The destination, AKA "the sink"**<br> &emsp;• Common sinks<br> &emsp;&emsp;• Another Kafka topic<br> &emsp;&emsp;• Iceberg<br> &emsp;&emsp;• Postgres|
| **Streaming Challenges**  | - ***Out of order events***  <br>- ***Late arriving data***<br>- ***Recovering from failures*** |
| **Out of order events**  | - How does Flink deal with out-of-order events? <br>&emsp;• **WATERMARKING** ensures there are no events newer than the watermark<br> &emsp;&emsp;• Give yourself a window for each event where over the next N seconds, it's considered OK there's a possiblility of out of order events, and Flink will fix it for you |
| **Late-Arriving Data**  | - How late is too late? <br> - Batch handles this mostly by waiting, although batch has issues around midnight UTC, too!<br>- Watermarking and late-arriging data are similar concepts<br> &emsp;• Watermarking is more for the 99% of data that arrives out of order in the window <br> &emsp;• Late-arriving data is more for the long tail of the small amount of data that comes in exceptionally late |
| **Recovering from failures**  | - When streaming pipelines fail, the longer it stays down, the more and more it gets backed up <br> - Flink manages this in a few ways:<br> &emsp;• Offsets<br> &emsp;&emsp;• When Flink starts up, you have to specify earliest offset, latest offset, or a specific moment in time <br> &emsp;&emsp;• Earliest offset<br> &emsp;&emsp;&emsp;• Means read everything in Kafka as far back as we can go<br> &emsp;&emsp;• Latest offset<br> &emsp;&emsp;&emsp;• Only read in new data after the job starts <br>&emsp;&emsp;• Specific timestamp (i.e. when it failed)<br> &emsp;&emsp;&emsp;• Only read results that match this timestamp or are newer <br> &emsp;• Checkpoints<br> &emsp;&emsp;• Internal to Flink, used for Flink itself<br> &emsp;&emsp;• You can tell Flink to checkpoint every N number of seconds and it saves the state of the job from that point in time<br>&emsp;&emsp;• Then Flink knows where to read from and where to write things to if things fail so when you start it up again, it doesn't re-read everything <br> &emsp;• Savepoints <br> &emsp;&emsp;• More like a CSV file used for systems other than Flink |

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- Which technology is primarily used for a continuous processing engine in streaming pipelines?
- What is a key characteristic of a streaming pipeline?
- How does Apache Flink handle out of order events?
- What is a benefit of batch processing over streaming processing in terms of data quality?
- When is streaming processing essential over other methods?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

Apache Flink is the best example of a streaming continuous processing engine, which processes data events immediately as they are generated. Streaming pipelines process data in a low-latency manner, meaning data is processed shortly after generation. This distinguishes them from traditional batch processing.

Apache Flink uses watermarks to define windows of time during which out of order events can be corrected, ensuring the accurate ordering of data. Batch processing has clear steps where data quality checks can easily be inserted, unlike streaming, which is continuous without distinct steps. Streaming processing is crucial for situations like fraud detection where immediate data processing is necessary to prevent or mitigate issues quickly.
