# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Apache Spark

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Managing Spark Jobs and Notebooks Day 2 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Lecture Contents**  | - Differences between Spark Server & Spark Notebook<br>- PySpark and ScholarSpark usage scenarios<br>- Implications of using UDFs <br>- Is a PySpark UDF inferior to ScholarSpark UDF?<br>- APIs <br> &emsp;• DataFrame API <br> &emsp;• Spark SQL API  <br> &emsp;• RDD API <br> &emsp;• DataSet API  <br> &emsp;• When to pick which API for what job|
| **Spark Server vs Spark Notebooks**  | - ***Spark Server*** (How AirBnB does it) <br> &emsp;• Session runs when you submit the job and it will run until it completes, when the session is killed <br> &emsp;• You have to submit your Spark job through the CLI <br> &emsp;&emsp;• Pass Spark Submit a Java class<br> &emsp;&emsp;• The Java class has a run method  <br> &emsp;• Usually, you upload a jar and the jar is run <br> &emsp;&emsp;• The jar can be pretty beefy if there are a lot of dependencies <br> &emsp;• Every run is fresh, things get uncached automatically<br> &emsp;• Nice for testing  <br> &emsp;• Sometimes slower than the Notebook method, sometimes better<br> &emsp;• The way it runs in the dev environment, it's going to run like that in production as well <br><br>- ***Notebook*** (how Netflix does it) <br> &emsp;• You have one Spark session that stays live and you have to terminate it later<br> &emsp;• Make sure to call `unpersist` |
| **Databricks Considerations**  | - Databricks should be connected with GitHub <br> &emsp;• PR review process for *every* change<br> &emsp;&emsp;• ***You can edit the notebook and the code will be running in production immediately!***<br> &emsp;&emsp;&emsp;• **USE CAUTION** when using notebooks, they aren't managed by Git and you can inadvertently be working with bad data if you don't know someone fiddled with the data earlier<br> &emsp;• CI/CD check |
| **Caching and Temporary Views** | - **Temporary views** <br> &emsp;• In Spark, when you have a dataframe, you can create a temporary view that is a lot like a CTE<br> &emsp;&emsp;• However, if you use them multiple times downstream, they will ***always get recomputed unless cached!*** <br><br>- **Caching**<br> &emsp;• Storage Levels <br> &emsp;&emsp;• `MEMORY_ONLY`<br> &emsp;&emsp;• `DISK_ONLY`<br> &emsp;&emsp;&emsp;• Materialized view and caching to disk are very similar in terms of performance<br> &emsp;&emsp;&emsp;• If you are using a lot of disk caching in Spark, you should be questioning if there's a staging table or if you should be breaking up your job into multiple, faster  steps <br> &emsp;&emsp;• `MEMORY_AND_DISK` (the default)<br> &emsp;• **If you have a large result set you are going to re-use multiple times, *a staging table* may be what you want**<br> &emsp;&emsp;• When you cache something to disk, you're writing it out as a table, so *when the Spark job terminates, that data disappears*<br> &emsp;&emsp;• If you use a staging table, you can re-use this data rather than throwing it away<br> &emsp;• Caching really only is good if it fits into memory <br> &emsp;&emsp;• Otherwise, there's probably a staging table in your pipeline you're missing!<br><br> &emsp;• **In notebooks:**<br> &emsp;&emsp;• *Call `unpersist` when you're done, otherwise **the cached data will just hang out!***|
| **Caching vs Broadcast**  | - Caching <br> &emsp;• Stores pre-computed values for re-use <br> &emsp;• Says partitioned<br> &emsp;&emsp;• You can save result sets this way <br>- Broadcast Join <br> &emsp;• Small data that gets cached and shipped in entirety to each executor<br> &emsp;• You get one partition with broadcast `JOINs` <br> &emsp;• More than a few GB are difficult to use broadcast `JOINs` with  |
| **Broadcast JOIN Optimization**  | <br>- One of the most important optimizations in Spark <br> &emsp;• For the most part, gets triggered automatically when it can when one side of the data is small<br> &emsp;&emsp;• The default definition of "small" is 10 MB <br> &emsp;&emsp;&emsp;• You can 200x this threshold, and Spark will still let you run the job if you have enough memory in your executors (limit yourself to single digit GBs) <br>- Broadcast `JOINs` prevent Shuffle <br> &emsp;• Remember, this is good! <br>- The threshold is set <br> &emsp;• `spark.sql.autoBroadcastJoinThreshold`<br> &emsp;&emsp;• If you don't want to set this threshold, you can use broadcast(df) instead <br>- You can ***explicitly wrap a dataset with broadcast(df)***, too! <br> &emsp;• ***This triggers the broadcast JOIN regardless of the size of the dataframe*** <br>- The little dataset doesn't shuffle around, it gets shipped to every executor because it's small enough to do this without getting split up|
| **UDFs**  | - **User-Defined Functions** <br> &emsp;• Allow you to do complex logic and processing <br>- PySpark quirks vs Scala <br> &emsp;• Python UDFs: <br> &emsp;&emsp; 1. Spark and Scala serializes the Python data <br> &emsp;&emsp; 2. Passes the data to a Python process <br> &emsp;&emsp; 3. Python deserializes and runs the code <br> &emsp;&emsp; 4. Python returns a serialized value back to Scala <br> &emsp;&emsp; 5. Scala gives you a result<br> &emsp;&emsp;• Can make Python UDFs less efficient<br> &emsp;&emsp;• Scala Spark gives you more performant UDAFs <br> &emsp;&emsp;&emsp;• Most people don't use UDAFs because of how niche they are <br>- Apache Arrow optimization in recent versions of Spark have helped PySpark UDFs become more inline with Scala Spark UDFs <br>- Dataset API allows you to not even need UDFs<br> &emsp;• You can create functional programming with your Scala Spark code <br> &emsp;&emsp;• You don't need to add a dataframe, just pure, functional data engineering <br> &emsp;• you can use pure Scala functions instead <br>- Zach recommends looking into using PySpark |
| **DataFrame vs Dataset vs SparkSQL**  | - **SparkSQL** is useful when you have many cooks in the kitchen <br> &emsp;• Lowest barrier to entry because you only need to know SQL to learn Spark <br><br>- **DataFrame** <br> &emsp;• DataFrame is nice because you can modularize the code and put stuff into separate functions, make them more testable, etc<br><br>- **Dataset** is Scala only! <br> &emsp;• Furthest toward software engineering because the dataset API gives you all of the end schema so you can create "mock data" more easily<br> &emsp;• You don't have to create any named Tuples because they already exist within the API<br> &emsp;&emsp;• Your unit and integration tests can just use the schema that is part of the pipeline to generate the fake data so you have a tighter integration <br> &emsp;• Handles `NULL` and nullability better <br> &emsp;&emsp;• You have to model whether a column is nullable or not, otherwise you will get errors (i.e. null point exception)<br> &emsp;&emsp;&emsp;• Great way to enforce quality control as well because if the pipeline ran, there are no null values <br><br>- DataFrame vs SparkSQL <br> &emsp;• ***DataFrame is more suited for pipelines that are more hardened and less likely to experience change***<br> &emsp;• ***SparkSQL is better for pipelines that are being iterated on quickly and in collaboration with data scientists***<br> &emsp;• Dataset is best for pipelines that require unit and integration tests <br>-***If you are building a pipeline for the long-haul and you're using Scala Spark, use the Dataset API***|
| **Parquet**  | - An amazing file format <br> &emsp;• Run-length encoding allows for powerful compression <br> &emsp;&emsp;• If you sort things effectively, the volume of the data is smaller and more efficient to query because you compressed it better<br>- Don't use global `.sort()`<br> &emsp;• Painful, slow, annoying <br> &emsp;• Incurs an extra Shuffle step with a partition of one <br> &emsp;&emsp;• This means you have to pass all the data to one executor so that it can guarantee the data is in the right order <br> &emsp;• Just say no to `.sort()`<br>- Use `.sortWithinPartitions`<br> &emsp;• Parallelizable, gets you good distribution<br> &emsp;• Looks at the data within the partition you are writing and will sort locally<br> &emsp;• Very fast<br> &emsp;• ***You don't have to pay an expensive cost to get the benefits of run-length encoding*** |
| **Spark Tuning**  | - **Executor Memory** <br> &emsp;• Don't just set to 16 GB and call it a day! It wastes a lot in the cloud <br>- Driver Memory <br> &emsp;• Only needs to be bumped if: <br> &emsp;&emsp;• You're calling `df.collect()` <br> &emsp;&emsp;• You have a very complex job! <br><br>- **Shuffle Partitions** <br> &emsp;• Default is 200 <br> &emsp;• Aim for ~100 MBs per partition to get the right sized output datasets<br> &emsp;• How to pick the right number of partitions: <br> &emsp;&emsp;• If your data is uniform or roughly uniform, you want between 100-200 MB per partition <br> &emsp;&emsp;• Gives you the most optimal throughput <br> &emsp;&emsp;• If your output dataset is 20 GB, 20 GB / 100 MB is ~2,000 <br> &emsp;&emsp;&emsp;• From there, choose numbers 50% lower and 50% higher to find the number of partitions <br> &emsp;&emsp;&emsp;&emsp;• In this example, try 1,000, 2,000, or 3,000 partitions to guage the performance of the job and choose the one with your preferred performance metrics <br><br>- **AQE (Adaptive Query Execution)** <br> &emsp;• Helps with skewed datasets<br> &emsp;• Wasteful if the dataset isn't skewed<br> &emsp;• *Spark will handle the skew for you!* |

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- In the context of Spark, what is a UDF?
- When might using Spark SQL be most beneficial?
- Why might PySpark UDFs perform worse than Scala Spark UDFs?
- What is a significant drawback of using Spark notebooks for production code deployment?
- What benefit does the DataFrame API offer over Spark SQL?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

In the context of Spark, a UDF is a user-defined function that allows a user to define custom functions for transformations within data frames. This is often utilized to perform complex logic. SparkSQL lowers the entry barrier for using Spark and is beneficial for collaboration with people who may not be familiar with programming but know SQL. A benefit of the DataFrame API over Spark SQL is the DataFrame API allows users to modularize code, making it more testable. This is an advantage over purely declarative SQL approaches.

PySpark UDFs involve significant serialization and deserialization between Python and JVM, which affects performance compared to Scala UDFS that run natively in JVM. Spark notebooks have a significant drawback when using them for production code deployment. Notebooks can be changed directly, leading to potential bad data entering production without proper version control and CI/CD processes.
