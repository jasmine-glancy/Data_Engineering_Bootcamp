# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Apache Spark

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Architecture, Optimization, and Best Practices Day 1 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Apache Spark**  | - Spark is a distributed compute framework that allows you to process very large amounts of data efficiently <br> &emsp;• The successor of Hadoop/ Java MapReduce<br><br>- **Benefits to Spark** <br> &emsp;• Spark leverages RAM much more effectively than previous iterations of distributed compute<br> &emsp;&emsp;• It's *way* faster than Hive/Java MR/etc<br> &emsp;• Spark is storage agnostic, allowing a decoupling of storage and compute<br> &emsp;&emsp;• Spark makes it easier to avoid vendor lock-in<br> &emsp;• Spark has a huge community of developers so StackOverflow/Chat GPT will help you troubleshoot <br><br>- **When Spark Isn't So Good**<br> &emsp;• Nobody else in the company knows Spark<br> &emsp;&emsp;• Spark is not immune to the bus factor<br> &emsp;• Your company already uses something else a lot<br> &emsp;&emsp;• Inertia is often times not worth it to overcome <br> &emsp;• Homogeneity of pipelines matters!|
| **How Spark Works**  | - The plan<br>- The driver<br>- The executors <br>- Using basketball as an analogy, the **plan** is the basketball play, the **driver** is the coach, and the **executors** are the basketball players |
| **The Plan**  | - The driver *tells the executor what to do* via the plan <br> &emsp;• This is the transformation you describe in Python, Scala, or SQL <br>- The plan is evaluated lazily <br> &emsp;• ***Lazy evaluation: Execution only happens when it needs to***<br>- When does execution "need to" happen? <br> &emsp;• Writing output <br> &emsp;• When part of the plan depends on the data itself <br> &emsp;&emsp;• (e.g. calling `dataframe.collect()` to ) |
| **The Driver**  | - The driver *reads the plan* <br>- The driver needs memory to process the plan <br>- Important Spark driver settings<br> &emsp;• `spark.driver.memory`<br> &emsp;&emsp;• The amount of memory the driver has to process the job <br> &emsp;&emsp;&emsp;• 2-16 GB range <br> &emsp;&emsp;• For complex jobs or jobs that use `dataframe.collect()`, you may need to bump this higher, or else *you'll experience out of memory*<br> &emsp;• `spark.driver.memoryOverheadFactor`<br> &emsp;&emsp;• What fraction the driver needs for non-heap related memory, usually 10%, might need to be higher for complex jobs<br>- The driver needs to determine a few things: <br> &emsp;• When to actually start executing the job and stop being lazy<br> &emsp;• How to `JOIN` datasets<br> &emsp;• How much parallelism each step needs after a `JOIN`<br> &emsp;&emsp;• Initial parallelism for those steps is how many files needs writing to |
| **The Executors**  | -The driver passes the plan to the executors <br>- AKA The executors do the actual work! <br>- `spark.executor.memory`<br> &emsp;• Should be the smallest number that runs all the time for a couple of days <br> &emsp;• Determines how much memory each executor gets <br> &emsp;• A low number here may cause Spark to "spill to disk" which will cause your job to be much slower<br>- `spark.executor.cores`<br> &emsp;• How many tasks can happen on each machine at once (default is 4, shouldn't go higher than 6) <br> &emsp;• If you have more than 6, you get a different bottle neck by the throughput of too many tasks running at the same time<br> &emsp;• Depending on the footprint of the tasks that are all running at once, you make yourself more prone to OOM<br>- `spark.executor.memoryOverheadFactor`<br> &emsp;• What percentage of memory should an executor use for non-heap related tasks, usually 10%<br> &emsp;•  For jobs with lots of UDFs and complexity, you may need to bump this up<br> &emsp;&emsp;• ***You can increase the memory overhead without increasing the memory itself!*** |
| **Types of JOINs in Spark**  | - Shuffle sort-merge `JOIN` <br> &emsp;• Generally speaking, you want to minimize these types of `JOINS`<br> &emsp;• Least performant, but the most versatile <br> &emsp;• Default `JOIN` strategy since Spark 2.3 <br>- Brodcast Hash `JOIN`<br> &emsp;• Works well if the left (or at least one) side of the join is small<br> &emsp;•`spark.sql.autoBroadcastJoinThreshold`<br> &emsp;&emsp;• Default is 10 MBs, can go as high as 8 GBs, but you will experience weird memory problems > 1 GBs<br> &emsp;• A `JOIN` without shuffle! <br>- Bucket `JOIN`s<br> &emsp;• A `JOIN` without shuffle! <br> &emsp;• ***Always bucket your tables on powers of two!***<br> &emsp;• Pick number of buckets based on the volume of your data |
| **How Shuffle Works**  | Stage 1. Use a map operation <br> &emsp;• i.e. `WHERE`, add new column, or `.with` for dataframe APIs<br> &emsp;• This is scalable to the number of files <br>Stage 2. Reduce <br><br> - Shuffle partitions and parallelism<br> &emsp;• `spark.sql.shuffle.partitions` and `spark.default.parallelism`<br> &emsp;• Just use `spark.sql.shuffle.partitions`!<br> &emsp;• `spark.default.parallelism` is related to the RDD API you should almost never be using! |
| **Is Shuffle Good or Bad?**  | - At low-to-medium volume <br> &emsp;• It's really good and makes our lives easier!<br>- At high volumes (i.e. > 10 TBs) <br> &emsp;• Painful!  |
| **Minimizing Shuffle at High Volumes**  | - Bucket the data if multiple `JOIN`s or aggregations are happening downstream<br>- Spark has the ability to bucket data to minimize or eliminate the need for shuffle when doing `JOIN`s<br>- Bucket `JOIN`s are very efficient, but have drawbacks<br>- Main drawback is the initial parallelism = number of buckets<br>- Bucket joins only work if the two tables number of buckets are multiples of each other  <br> &emsp;• ***This is why we always use powers of 2 for our bucket numbers!***<br>- Presto can be strange with bucketed joins because the initial parallelism will always be 16 |
| **Shuffle & Skew**  | - Sometimes some partitions have dramatically more data than others. This can happen because: <br> &emsp;• Not enough partitions<br> &emsp;• The way the data is<br> &emsp;&emsp;• i.e. Beyonce gets a lot more notifications than the average Facebook user <br>- **How to tell if your data is skewed:**<br> &emsp;• Most common is a job getting to 99%, taking forever, and failing<br> &emsp;• Another, more scientific way is to do a box-and-whiskers plot of the data to see if there's any extreme outliers |
| **Ways to Deal With Skew**  | - Adaptive query execution - *only in Spark 3+* <br> &emsp;• Set `spark.sql.adaptive.enabled = True`<br>-Salting the `GROUP BY` - best option before Spark 3<br> &emsp;• `GROUP BY` a random number<br> &emsp;&emsp;• Gives you a partial aggregation which makes the skewed data smaller <br> &emsp;• Aggregate + `GROUP BY` again<br> &emsp;&emsp;• Remove the random number and sum everything again <br> &emsp;• Be careful with things like `AVG` - break it into `SUM` and `COUNT` and divide! <br> &emsp;&emsp;• [Example](#salting-the-group-by)<br>- Filter out the outliers <br>- Partition your downstream table where one side processes the majority, but the other side processes the outliers |
| **Spark on Databricks vs Regular Spark**  | - Managed Spark (i.e. Databricks) <br> &emsp;• Should you use notebooks? <br> &emsp;&emsp;• ***YES!*** <br> &emsp;• How to test job? <br> &emsp;&emsp;• Run the notebook <br> &emsp;• Version control <br> &emsp;&emsp;• Git or Notebook versioning <br>- Unmanaged Spark (i.e. Big Tech) <br> &emsp;• Should you use notebooks?<br> &emsp;&emsp;• Only for proof of concepts<br> &emsp;• How to test job?<br> &emsp;&emsp;• Spark-submit from CLI<br> &emsp;• Version control<br> &emsp;&emsp;• Git |
| **How Can Spark Read Data**  | - From the lake <br> &emsp;• Delta Lake, Apache Iceberg, Hive metastore <br>- From an RDBMS <br> &emsp;• Postgres, Oracle, etc <br> - From an API <br> &emsp;• Make a REST call and turn into data<br> &emsp;&emsp;• Be careful because this usually happens on the Driver!<br> - From a flat file<br> &emsp;• i.e. csv, json |
| **Spark Output Dataset**  | - Should almost always be partitioned on "date" <br> &emsp;• This is the execution date of the pipeline <br> &emsp;• In big tech this is called "ds partitioning" |

### Salting the GROUP BY

```sql
df.withColumn("salt_random_column", (rand * n).cast(IntegerType))
    .groupBy(groupByFields, "salt_random_column")
    .agg(aggFields)
    .groupBy(groupByFields)
    .agg(aggFields)
```

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What is the primary advantage of Apache Spark over its predecessors like Hadoop MapReduce?
- In Apache Spark, what role does the 'Driver' play?
- Which of the following is a best practice when setting the 'executor memory' for Apache Spark?
- When is it preferable to use a 'Broadcast Hash Join' in Spark?
- Which feature in Spark 3 helps in automatically managing skew issues during execution?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

Compared to its predecessors (i.e. Hadoop and MapReduce), Apache Spark minimizes disk input/output by storing intermediate data. Hadoop and MapReduce requires writing after every step, which makes Spark faster and more efficient. In Apache Spark, the driver acts as the coach, managign plans and executions. The driver constructs the logical plan and coordinates execution by assigning tasks to executors. It's important to find a balance to prevent memory waste while also avoiding "out of memory" errors.  The best practice when setting the executor memory for Apache Spark is to adjust it based on a trial to get the optimum executor memory with minimal overhead.

It's preferable to use a broadcast hash join when one side of the join is small. In such cases, the small dataset can be broadcasted to all executors, eliminating the need for a full shuffle. `spark.sql.adaptive.enabled` is a Spark 3 feature that allows Spark to automatically handle skewed partitions effectively by analyzing and adjusting execution plans.
