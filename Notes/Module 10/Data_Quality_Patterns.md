# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Data Quality Patterns

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> MIDAS Process from Airbnb Day 1 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Rule of Thumb**  | - ***Good pipelines start with good documentation*** |
| **Working With Bad Data**  | - Symptoms include <br> &emsp;• Rage<br> &emsp;• Irritability<br> &emsp;• Insomnia |
| **Data Quality**  | - What is data quality? <br> &emsp;• DISCOVERABLE! <br> &emsp;&emsp;• If someone is trying to make a decision, and they are trying to make sure this data exists, it is easy to find it<br> &emsp;• Misunderstood/incomplete definitions of data quality<br> &emsp;&emsp;• Think about the gaps!<br> &emsp;&emsp;• Is there anything missing from the data set?<br> &emsp;&emsp;• **Black swan events** are difficult to predict for machine learning modules(i.e. the COVID-19 pandemic)<br> &emsp;• There aren't any `NULL`s or duplicates<br> &emsp;• There is business value being derived from the data<br> &emsp;&emsp;• Generally speaking, every single data pipeline you write is either going to generate or save money in some way<br> &emsp;• The data is easy to use<br> &emsp;&emsp;• The column names are obvious and make sense<br> &emsp;&emsp;• dim = dimension columns <br> &emsp;&emsp;• m = metric columns<br> &emsp;• The data arrives in a timely manner<br> &emsp;&emsp;• Come up with agreed upon arrival times and refresh rates to streamline the process<br>- ***Data quality = data trust + data impact*** |
| **Building High Trust in the Data Sets You Build**  | - Before you start coding, *lean into empathy*!<br> &emsp;• Gather the stakeholders <br> &emsp;• "If cost and time was not a barrier, what data would you want?"<br>&emsp;&emsp;• Gives you clarity on the problems they are trying to solve<br>&emsp;&emsp;• Gives you ideas on what to do down the line <br>- A spec review of pipeline design <br>&emsp;• Every spec should be reviewed by another data engineer and a stakeholder <br>&emsp;&emsp;• Helps cover most of the requirements<br>- Clarify the business impact *before* coding! <br>- Ask downstream stakeholders about all current and future needs<br>&emsp;• Stakeholders will usually have a burning question they want answered ASAP <br>&emsp;&emsp;• What you build should help find them answers now and six months from now<br>- Follow the AirBnB MIDAS process<br> &emsp;• Even one or two steps in the process can make a huge difference! |
| **AirBnB MIDAS Process**  | 1. Design Spec <br> &emsp;• "This is the pipeline we are going to build, and this is how we build it"<br>2. Spec Review <br> &emsp;• Technical and stakeholder <br>3. Build & Backfill Pipeline <br> &emsp;• Write the Spark/Airflow code <br> &emsp;• One month backfill first so your analysts do step 4<br>4. SQL Validation <br> &emsp;• Done by data analysts <br> &emsp;• Needs to happen by someone who isn't you to ensure bugs don't hide<br>5. Minerva Validation <br> &emsp;• Looking at the metrics via an open-source metric repository container<br>6. Data Review + Code Review<br> &emsp;• Make sure there aren't any modeling problems<br> &emsp;• Check for unit tests and integration tests<br>7. Minerva Migration<br>8. Minerva Review<br>9. Launch PSA<br><br>- **Why all this upfront work?**<br>&emsp;• **Pre-build** stakeholder buy-in dramatically increases data trust! <br>&emsp;&emsp;• They give you their opinions<br>&emsp;• They feel like they have skin in the game! <br>&emsp;&emsp;• They're a part of the process<br>&emsp;• *This prevents painful backfills when you miss a requirement* <br>&emsp;&emsp;• Reduces the potential for miscommunications <br>- Stakeholder meetings are a unironically fun way to solve business problems together <br><br> **When should you do this heavy process?**<br>&emsp;• Are important decisions being made on this data? <br>&emsp;&emsp;• If it's exploratory, it doesn't need all nine steps<br>&emsp;• Is this data not going to change much over the next year?<br>&emsp;&emsp;• If the data is going to change a lot, then it may not be worth it to do all the steps until the next change<br>&emsp;• The nine step process is for multi-year master data that won't be subjected to very disruptive change<br>&emsp;&emsp;• Or if it is subjected to disruptive change, it happens on a couple-year basis <br>&emsp;• Remember, sometimes **not** doing the heavy process provides more value |
| **Building Good Data Documentation**  | - People can learn about what you are creating faster |
| **Data Quality Checks**  | - How are they different between facts and dimensions? <br><br>- **Basic checks**  <br> &emsp;• Not `NULL`, is there data, no duplicates, make sure enum values are all valid (data lake)<br> &emsp;• Every pipeline should have these basic checks all the time<br>- **Intermediate checks** <br> &emsp;• Row count looks good, week-over-week row count cut by dimensions looks good <br> &emsp;• Week-over-week gives you a better trend idea <br> &emsp;• Intermediate checks usually fail on Christmas because of seasonality<br> &emsp;• Are business rules enforced? <br>- **Advanced checks** <br> &emsp;• Seasonality adjusted row counts looks good <br> &emsp;• Can be expensive to bring in<br> &emsp;&emsp;• As your data infrastructure improves, generally speaking advanced checks become easier to add|
| **Good Design Spec Ingredients**  | - Description <br> &emsp;• Why are you building this? <br>- Flow Diagrams<br> &emsp;• How things position <br> &emsp;• How data goes from raw data to fact/dimension data and metrics<br>- Schemas<br> &emsp;• DDL statements <br> &emsp;• `CREATE TABLE` statements <br> &emsp;• Column comments to discuss oddities<br>- Quality checks<br>- Metric Definitions<br> &emsp;• What are you trying to measure? <br>- Example Queries<br> &emsp;• Helpful if you have `STRUCT`s and `ARRAY`s |
| **Good Flow Diagram Example**  | ![Flow Diagram](flow_diagram.png) <br>- fct_activity is master data<br>&emsp;• Most people will be querying this <br>- agg_activity <br>&emsp;• Aggregate this to get granulated metrics <br>- Lucid Chart is a way you can make these <br> &emsp;• Google Drawing is another way|
| **What Does a Good Schema Look Like?**  | - Good names <br> &emsp;• Probably fct, dim, scd, or agg in the name to demonstrate the type of table it is!<br>- Column comments on every column<br>- Follow the naming conventions your company chooses!<br>- If they don't have naming conventions, there's a spot for huge impact!<br> &emsp;• Figure out if there is a reason |
| **Dimensioal Quality Checks**  | - Dimensional tables usually: <br> &emsp;• Grow or are flat day-over-day <br> &emsp;&emsp;• Table is growing check is very common (i.e. are there more rows today than there were yesterday?)<br> &emsp;• Don't grow sharply<br> &emsp;&emsp;• Percent difference from last week should be small<br> &emsp;• Have complex relationships that should be checked <br> &emsp;&emsp;• Think foreign keys in a relational world <br> &emsp;&emsp;• Sometimes connections have to exist, and if they do, they should be checked|
| **Fact Quality Checks**  | - Have seasonality to them <br> &emsp;• Week-over-week is much better than day-over-day<br>- Affected by holidays<br> &emsp;• Christmas and New Years false positive a lot<br>- Can grow and shrink<br> &emsp;• Table is Growing is not a good check here <br>- More prone to duplicates <br> &emsp;• Deduplicate check is a must<br> &emsp;&emsp;• If you're using Presto, use `APPROX_COUNT_DISTINCT` instead of `COUNT(DISTINCT)` for deduped checks <br> &emsp;&emsp;&emsp;• It's 99.9% the same, but dramatically more efficient<br>- More prone to `NULL`s and other row-level quality issues<br>- Have references to entities that should exist<br> &emsp;• Similar to FK checks in relational |

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What is the primary goal of the MIDAS process at Airbnb?
- What is emphasized as critical in building trust in data sets?
- Why should quality checks avoid using 'count distinct' in Presto?
- When is it appropriate to skip the full MIDAS process?
- Which step is emphasized before creating a data pipeline in MIDAS?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

Before creating a pipeline, you should obtain stakeholder input and clarify the spec. This helps ensure the pipeline will meet all user requirements and have a higher likelyhood of success. The MIDAS process is designed to ensure data pipelines are durable, understandable, and improve over time, maximizing their long-term value. Implementing a full quality process may not be beneficial for data that frequently changes. It's better to wait until the changes stabalize.

Proper documentation essential for others to understand, debug, and improve data pipelines. This, in turn, helps build trust in your data sets. If you are using Presto, `COUNT(DISTINCT)` can cause performance issues. `APPROX_COUNT_DISTINCT` is more efficient and provides similar accuracy.
