# <img src="../books.svg" alt="Stack of red books with a graduation cap on top, symbolizing education and achievement, set against a plain background" width="30" height="20" /> Data Pipeline Maintenance

## <img src="../notes.svg" alt="Orange pencil lying diagonally on a white sheet of paper, representing note taking and documentation, with a clean and organized appearance" width="20" height="15" /> Strategies for Maintenance and Dock Building Day 2 Lecture

| Concept                | Notes            |
|---------------------|------------------|
| **Signals of Tech Debt in Data Engineering**  | - Painful pipelines that break or are delayed<br>- Large cloud bills <br>- Multiple sources of truth<br>- Unclear data sets that aren't documented|
| **Path Forward for Painful Pipelines**  | - The only thing better than optimized is deprecated!<br>- Is there anything that can be done? <br> &emsp;• Migrate to new technology<br> &emsp;&emsp;• Hive to Spark migration? <br> &emsp;&emsp;&emsp;• Often sees an order of magnitude and increase in efficiency. Especially for pipelines that use high-cardinality `GROUP BY` and/or `JOIN`<br> &emsp;• Implement better data modeling<br> &emsp;• Bucketing?<br> &emsp;• Sampling?<br>- Deprecating without a replacement can be a hard sell!  |
| **Sampling**  | - When should you sample? <br> &emsp;• If directionality is what you care about, sampling is good <br>&emsp;• Make sure to consult a data scientist on this to guarantee you get the best sample of the data set <br>&emsp;• You want a random sample <br>- When should you not sample? <br>&emsp;• If you need the entire data set for auditing, sampling won't work! |
| **Bucketing**  | - You should consider bucketing when <br> &emsp;• You have an expensive high-cardinality `JOIN` or `GROUP BY` <br>- ***Shuffle:*** When you `JOIN` two data sets, the `JOIN` key gets hashed as a number and gets %'d out. The remainder is the partition the row is assigned to. <br>&emsp;• The default modulo in Spark is 200 <br>- You can write the left and right data sets to their own partitions<br>- An important optimization techniques for the highest volumes of data<br>&emsp;• Writing the left and right side to bucketed tables allow you to `JOIN` without shuffle |
| **Large Cloud Bills**  | - IO (Input/Output) is usually the number one cloud cost! <br> &emsp;• Signal tables can help here <br> &emsp;• WAP does a partition exchange, so every time you write a table, you pay your IO costs twice <br>- Followed by compute <br> &emsp;• Fixed cost<br>- Then storage <br> &emsp;• Cheap in comparison to IO and Compute <br>- If your pipeline is producing data that will be visualized in a dashboard and it is a final aggregate layer, your biggest cost will probably be in **compute** and then storage <br><br>- Too much IO can happen because <br> &emsp;• Duplicative data models  <br> &emsp;• Inefficient pipelines<br> &emsp;&emsp;• Use cumulative design when possible <br> &emsp;• Excessive backfills <br> &emsp;• Not sampling  <br> &emsp;• Not subpartitioning your data correctly<br> &emsp;&emsp;• Predicate pushdown is your friend<br> &emsp;&emsp;• Subpartitions allow you to avoid reading in a big chunk of data you don't nee to read in <br>- **Large IO and compute costs** are correlated by:<br> &emsp;• Scanning too much data (use cumulative tables)<br> &emsp;• O(n^2) algorithms applied in UDFs (nested loops are usually bad)<br>- **Large IO and storage costs** are correlatd by: <br> &emsp;• Not leveraging Parquet file format effectively <br> &emsp;• Duplicative data models|
| **Why Subpartitions Minimize IO**  | ![subpartitions](subpartitions.png) - Subpartition on `date` and `channel`<br> &emsp;• When this query runs in Spark, it will skip all folders except for the ones highlighted in green. <br> &emsp;• This helps reduce IO costs <br>- Subpartitions should have 30 or less values |
| **Multiple Sources of Truth**  | - Some of the hardest but most impactful work for data engineers<br>- **Steps** <br> &emsp;• Document all sources and the discrepancies <br> &emsp;&emsp;• Talk with all relevent stakeholders!<br> &emsp;&emsp;• If you can code search for similar names, that works great<br> &emsp;&emsp;• Lineage of the shared data is a good place to start, too <br> &emsp;• Understand from stakeholders why they needed something different <br> &emsp;&emsp;• There's usually a reason why there's multiple data sets <br> &emsp;&emsp;• Could be organizational, ownership, technical, skills, etc <br> &emsp;&emsp;• Work with stakeholders to find an ownership model that works<br> &emsp;• Build a spec that outlines a new path forward that can be agreed upon <br> &emsp;&emsp;• Capture all the needs from the stakeholders <br> &emsp;&emsp;• Get all the multiple sources of truth owners to sign off<br> &emsp;&emsp;&emsp;• Helps with migration later|
| **Models For Getting Ahead of Tech Debt**  | 1. Fix "as you go"<br> &emsp;• Fix the tech debt as you implement new features <br> &emsp;• *Pros* <br> &emsp;&emsp;• There isn't much incremental burden<br> &emsp;• *Cons*<br> &emsp;&emsp;• Tech debt rarely actually gets tackled because it's the last priority<br><br>2.  Allocate a portion of time each quarter<br> &emsp;• *Pros* <br> &emsp;&emsp;• You fix things in big bursts <br> &emsp;&emsp;• You actually fix a lot of problems <br> &emsp;• *Cons*<br> &emsp;&emsp;• You tie up the team for a week <br> &emsp;&emsp;• Tech debt builds up throughout the quarter <br><br>3. Have the on-call person focus on tech debt during their shift  <br> &emsp;• *Pros* <br> &emsp;&emsp;• They fix bugs as they arise<br> &emsp;• *Cons*<br> &emsp;&emsp;• On-call person does nothing else that week |
| **Data Migration Models**  | - The extremely cautious approach <br> &emsp;• Be careful not to break anything <br> &emsp;• Parallel pipelines for months<br>- The bull in a china shop approach<br> &emsp;• Efficiency wins, minimize risk by migrating high-impact pipelines first <br> &emsp;• Kill the legacy pipeline as soon as you can |
| **Proper On-call Responsibilities**  | - Set proper expectations with your stakeholders!<br>- ***Document every failure and bug*** <br> &emsp;• You will thank yourself later! <br>- On-call handoff <br> &emsp;• Should be a 20-30 min sync to pass context from one on-call to the next |
| **Runbooks**  | - Complex pipelines need runbooks<br> &emsp;• Pipelines with many inputs, outputs used by a lot of teams, complex logic, a lot of data quality checks, etc <br> &emsp;• Runbooks should be linked in the spec<br>- ***Most important pieces***<br> &emsp;• Primary and secondary owners <br> &emsp;&emsp;• **Primary owner** owns the pipeline and should be contacted to help troubleshoot if you need help when on call <br> &emsp;&emsp;• If the primary owner is on vacation, tap the **secondary owner** <br> &emsp;• Upstream owners <br> &emsp;&emsp;• Helpful to provide information in the event the upstream data fails <br> &emsp;&emsp;• *Team ownership changes less than individuals* <br> &emsp;&emsp;• **Teams, not individuals** <br> &emsp;• Common issues (if applicable)<br> &emsp;• Critical downstream owners <br> &emsp;&emsp;• **Teams, not individuals**<br> &emsp;• SLAs and agreements <br>- ***Runbooks vs Specs*** <br> &emsp;• Can sometimes be referred to interchangably|
| **Example Flow**  | ![Runbook](runbook.png) <br>- You should have regularly occurring meetings with people on both sides of the stream!<br> &emsp;• Monthly or quarterly one-on-ones with owners and groups |
| **Example Runbook**  | - On-call runbook for EcZachly Inc Growth Pipeline <br> &emsp;• **Primary owner:** Zach <br> &emsp;• **Secondary owner:** Lulu <br><br> **Common Issues**<br> &emsp;• Upstream datasets <br> &emsp;&emsp;• Web site events <br> &emsp;&emsp;&emsp;• Common anomalies <br> &emsp;&emsp;&emsp;&emsp;• Sometimes referrer is `NULL` too much, this is fixed downstream but we are alerted about it because it messes with the analytics<br> &emsp;&emsp;• User database exports <br> &emsp;&emsp;&emsp;• Export might fail to be extracted on a given day, when this happens, use yesterday's export for today <br> &emsp;&emsp;&emsp;&emsp;• *Remember, this is not idempotent* <br> &emsp;• Downstream consumers <br> &emsp;&emsp;• Experimentation platform <br> &emsp;&emsp;• Dashboards <br><br>**SLAs**<br>The data should land 4 hours after UTC midnight|

## <img src="../question-and-answer.svg" alt="Two speech bubbles, one with a large letter Q and the other with a large letter A, representing a question and answer exchange in a friendly and approachable style" width="35" height="28" /> Cues

- What is a primary reason for deprecating a data pipeline according to the lecture?
- Why might a company choose to use aggressive data migration over cautious migration?
- What is a potential benefit of using sampling in data pipelines?
- When is it not advisable to use sampling as a strategy in pipeline maintenance?
- Which is a common indicator of tech debt in data engineering mentioned in the lecture?
- What should you consider when deciding whether to deprecate a data pipeline?
- What is a major advantage of migrating from Hive to Spark for certain types of data pipelines?
- Why is sampling considered as a strategy to improve pipeline performance?
- What is the primary downside of the 'dedicated time' approach in managing tech debt?
- What should be documented in a data pipeline's runbook?

---

## <img src="../summary.svg" alt="Rolled parchment scroll with visible lines, symbolizing a summary or conclusion, placed on a neutral background" width="30" height="18" /> Summary

Having multiple sources of data with conflicting definitions creates alignment issues, representing a significant form of tech debt. There are several ways to manage tech debt, which each have pros and cons.

Deprecating unnecessary pipelines reduces operational and maintenance costs, especially saving on cloud expenses. A company may choose to use aggressive data migration over cautious migration because the aggressive migration strategy forces stakeholders to adopt new pipelines quickly. This reduces costs and the duration of maintaining parallel systems. A pipeline that frequently breaks, fails, or has a high maintenance cost is a candidate for deprecation.

Sampling reduces the amount of data processed, which minimizes computational demand without significantly affecting overall data insights. Sampling can be suitable for certain analytical tasks. However, it's not advisable to use sampling as a strategy in pipeline maintenance when the entire dataset is necessary for auditing purposes because it omits certain data points.

Spark can provide major efficiency gains due to its RAM usage, especially for pipelines using high cardinalty GROUP BY or JOIN operations, making it much faster compared to disk-based processing in Hive. Runbooks and specs are a good way to maintain pipelines. A runbook documents common pipeline issues, SLAs, and primary and secondary owners. The goal is to facilitate troubleshooting, establish expectations, and identify the right contact persons during failures.
